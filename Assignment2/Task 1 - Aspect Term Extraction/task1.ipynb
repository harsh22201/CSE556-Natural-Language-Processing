{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10934341,"sourceType":"datasetVersion","datasetId":6782041}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"import json\nimport re\n\ndef preprocess(input_file, output_file):\n    # Read the JSON file\n    with open(input_file, 'r', encoding='utf-8') as f:\n        input_data = json.load(f)\n    \n    output_data = []\n    \n    for sentence_data in input_data:\n        sentence = sentence_data['sentence'].lower() \n        sentence = re.sub(r'[^a-zA-Z0-9\\s]', ' ', sentence)  # Remove punctuation and special characters\n        tokens = sentence.split()\n        labels = ['O'] * len(tokens)\n        aspect_terms = []\n        \n        # Process aspect terms\n        for aspect in sentence_data.get('aspect_terms', []):\n            term = aspect['term'].lower()\n            term = re.sub(r'[^a-zA-Z0-9\\s]', ' ', term)  # Remove punctuation and special characters\n            term_tokens = term.split()\n            aspect_terms.append(term)\n            \n            # Assign BIO labels\n            start_idx = tokens.index(term_tokens[0])  # Find first token index\n            labels[start_idx] = 'B'\n            for i in range(1, len(term_tokens)):\n                labels[start_idx + i] = 'I'\n        \n        output_data.append({\n            'sentence': sentence,\n            'tokens': tokens,\n            'labels': labels,\n            'aspect_terms': aspect_terms\n        })\n    \n    # Write to output JSON file\n    with open(output_file, 'w', encoding='utf-8') as f:\n        json.dump(output_data, f, indent=4)\n\n\npreprocess(\"/kaggle/input/nlp-a2/train.json\", \"train_task_1.json\")\npreprocess(\"/kaggle/input/nlp-a2/val.json\", \"val_task_1.json\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:29:31.531927Z","iopub.execute_input":"2025-03-16T10:29:31.532301Z","iopub.status.idle":"2025-03-16T10:29:31.706725Z","shell.execute_reply.started":"2025-03-16T10:29:31.532262Z","shell.execute_reply":"2025-03-16T10:29:31.705821Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Pre-Trained Word Embeddings","metadata":{}},{"cell_type":"code","source":"import torch\n\nclass WordEmbeddings:\n    PAD_IDX = 0\n    UNK_IDX = 1\n    def __init__(self, file_path, skip_first=False):\n        \"\"\"\n        Args:\n            file_path (str): Path to the word embedding file (GloVe or FastText).\n            skip_first (bool): Whether to skip the first line (needed for FastText).\n        \"\"\"\n        self.idx2word, self.embeddings = self.load_embeddings(file_path, skip_first)\n\n        # Add special tokens <PAD> (index = 0) and <UNK> (index = 1)\n        self.idx2word = [\"<PAD>\", \"<UNK>\"] + self.idx2word\n\n        # Convert embeddings to a tensor\n        self.embeddings = torch.as_tensor(self.embeddings, dtype=torch.float32)\n\n        # Generate word2idx mapping\n        self.word2idx = {word: idx for idx, word in enumerate(self.idx2word)}\n\n        # Append new embeddings (zeros for PAD, small random for UNK)\n        pad_embedding = torch.zeros((1, self.embeddings.shape[1]))  # <PAD> = all zeros\n        unk_embedding = torch.rand((1, self.embeddings.shape[1])) * 0.01  # Small random values for <UNK>\n        self.embeddings = torch.cat([pad_embedding, unk_embedding, self.embeddings], dim=0)\n\n    def load_embeddings(self, file_path, skip_first):\n        \"\"\"Loads embeddings from a file.\"\"\"\n        idx2word = []\n        embeddings = []\n\n        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n            if skip_first:\n                next(f)  # Skip first line \n            for line in f:\n                values = line.strip().split()\n                word = values[0]\n                vector = list(map(float, values[1:]))\n                idx2word.append(word)\n                embeddings.append(vector)\n\n        return idx2word, embeddings\n\n    def get_embedding(self, word):\n        \"\"\"Returns the embedding for a given word or the <UNK> embedding if not found.\"\"\"\n        idx = self.word2idx.get(word, self.word2idx[\"<UNK>\"])\n        return self.embeddings[idx]\n\n    def __len__(self):\n        \"\"\"Returns the vocabulary size.\"\"\"\n        return len(self.idx2word)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:29:31.707816Z","iopub.execute_input":"2025-03-16T10:29:31.708106Z","iopub.status.idle":"2025-03-16T10:29:34.845660Z","shell.execute_reply.started":"2025-03-16T10:29:31.708080Z","shell.execute_reply":"2025-03-16T10:29:34.845022Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"### GloVe and fastText","metadata":{}},{"cell_type":"code","source":"glove_path = \"/kaggle/input/nlp-a2/glove.6B/glove.6B.300d.txt\"\nGloVe = WordEmbeddings(glove_path, skip_first=False) \n\nfasttext_path = \"/kaggle/input/nlp-a2/wiki-news-300d-1M.vec/wiki-news-300d-1M.vec\"\nfastText = WordEmbeddings(fasttext_path, skip_first=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:29:34.847055Z","iopub.execute_input":"2025-03-16T10:29:34.847433Z","iopub.status.idle":"2025-03-16T10:31:49.925311Z","shell.execute_reply.started":"2025-03-16T10:29:34.847381Z","shell.execute_reply":"2025-03-16T10:31:49.924623Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Aspect Term Extraction (ATE) Dataset using BIO encoding","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\nclass ATE_Dataset(Dataset):\n    # BIO Label Mapping\n    BIOlabel = {\"B\": 0, \"I\": 1, \"O\": 2}\n    PADlabel = -100\n    \n    def __init__(self, filepath, word_embeddings):\n        \"\"\"\n        Args:\n            filepath (str): Path to the JSON file containing BIO-tagged data.\n            word_embeddings (WordEmbeddings): Preloaded WordEmbeddings object.\n        \"\"\"\n        self.word_embeddings = word_embeddings\n\n        # Load and Process JSON\n        self.data = []\n        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n            samples = json.load(f)\n\n        for sample in samples:\n            tokens_idx = [\n                self.word_embeddings.word2idx.get(token, self.word_embeddings.word2idx[\"<UNK>\"])\n                for token in sample[\"tokens\"]\n            ]\n            labels_idx = [ATE_Dataset.BIOlabel[label] for label in sample[\"labels\"]]\n\n            self.data.append((torch.tensor(tokens_idx, dtype=torch.long),\n                              torch.tensor(labels_idx, dtype=torch.long)))\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        \"\"\"Returns token indices and label indices for one sample.\"\"\"\n        return self.data[idx]\n\n    @staticmethod\n    def collate_fn(batch):\n        \"\"\"\n        Custom collate function to pad sequences to the max length in a batch.\n        \"\"\"\n        tokens, labels = zip(*batch)  # Unpacking list of tuples\n    \n        # Get max sequence length\n        max_len = max(len(t) for t in tokens)\n    \n        # Padding\n        pad_idx = 0  #  <PAD> has index 0\n        tokens_padded = [torch.cat([t, torch.full((max_len - len(t),), WordEmbeddings.PAD_IDX, dtype=torch.long)]) for t in tokens]\n        labels_padded = [torch.cat([l, torch.full((max_len - len(l),), ATE_Dataset.PADlabel, dtype=torch.long)]) for l in labels]\n    \n        return torch.stack(tokens_padded), torch.stack(labels_padded)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:31:49.926386Z","iopub.execute_input":"2025-03-16T10:31:49.926634Z","iopub.status.idle":"2025-03-16T10:31:49.958367Z","shell.execute_reply.started":"2025-03-16T10:31:49.926613Z","shell.execute_reply":"2025-03-16T10:31:49.957390Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Models\n###  RNN","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass ATE_RNN(nn.Module):\n    def __init__(self, pretrained_embeddings, hidden_dim, output_dim):\n        super(ATE_RNN, self).__init__()\n        vocab_size, embed_dim = pretrained_embeddings.shape\n        \n        # Load pretrained embeddings\n        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True, padding_idx=WordEmbeddings.PAD_IDX)\n        \n        # RNN layer\n        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n        \n        # Fully connected layer\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        x = self.embedding(x)  # Convert indices to word embeddings\n        h, _ = self.rnn(x)   # Pass through RNN\n        out = self.fc(h)     # Pass hidden state to Fully connected layer for classification\n        return out  # Output shape: (batch, seq_len, num_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:31:49.959381Z","iopub.execute_input":"2025-03-16T10:31:49.959688Z","iopub.status.idle":"2025-03-16T10:31:49.980301Z","shell.execute_reply.started":"2025-03-16T10:31:49.959657Z","shell.execute_reply":"2025-03-16T10:31:49.979605Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### GRU","metadata":{}},{"cell_type":"code","source":"class ATE_GRU(nn.Module):\n    def __init__(self, pretrained_embeddings, hidden_dim, output_dim):\n        super(ATE_GRU, self).__init__()\n        vocab_size, embed_dim = pretrained_embeddings.shape\n        \n        # Load pretrained embeddings\n        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True, padding_idx=WordEmbeddings.PAD_IDX)\n        \n        # GRU layer\n        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n        \n        # Fully connected layer\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        x = self.embedding(x)  # Convert indices to word embeddings\n        h, _ = self.gru(x)   # Pass through GRU\n        out = self.fc(h)     # Pass hidden state to Fully connected layer for classification\n        return out  # Output shape: (batch, seq_len, num_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:31:49.981084Z","iopub.execute_input":"2025-03-16T10:31:49.981355Z","iopub.status.idle":"2025-03-16T10:31:50.000247Z","shell.execute_reply.started":"2025-03-16T10:31:49.981324Z","shell.execute_reply":"2025-03-16T10:31:49.999472Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.optim as optim\nimport wandb\nfrom kaggle_secrets import UserSecretsClient\n\n# Login to W&B\nuser_secrets = UserSecretsClient()\nwandb_api = user_secrets.get_secret(\"WANDB_API_KEY\")\nwandb.login(key=wandb_api)\n\n# Define Device (GPU if available, else CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef train(model, train_loader, val_loader, epochs, learning_rate, model_name=\"Model\"):\n    \"\"\"\n    Train a sequence labeling model for BIO tagging and log metrics to wandb.\n\n    Args:\n        model (nn.Module): The PyTorch model (GRU, RNN, etc.).\n        train_loader (DataLoader): DataLoader for training data.\n        val_loader (DataLoader): DataLoader for validation data.\n        epochs (int): Number of training epochs.\n        learning_rate (float): Learning rate.\n        model_name (str): Name of the model for logging.\n    \"\"\"\n\n    # Initialize WandB\n    wandb.init(project=\"NLP_A2_Task1\", name=model_name)\n    wandb.config.update({\n        \"epochs\": epochs,\n        \"learning rate\": learning_rate,\n        \"batch_size\": train_loader.batch_size,\n        \"loss_function\": \"CrossEntropyLoss\",\n        \"optimizer\": \"Adam\"\n    })\n\n    model = model.to(device)  # Move model to GPU/CPU\n    criterion = nn.CrossEntropyLoss(ignore_index=ATE_Dataset.PADlabel)  # Ignore padding token in loss\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Adam Optimizer\n\n    print(f\"------------------------------------- TRAINING {model_name} -------------------------------------\")\n    for epoch in range(epochs):\n        # Train Phase\n        model.train()\n\n        for tokens, labels in train_loader:\n            tokens, labels = tokens.to(device), labels.to(device)\n\n            optimizer.zero_grad()  # Reset gradients\n            outputs = model(tokens)  # Forward pass\n\n            # Reshape outputs & labels for loss calculation\n            loss = criterion(outputs.view(-1, outputs.shape[-1]), labels.view(-1))\n            loss.backward()  # Backpropagation\n            optimizer.step()  # Update weights\n            \n       # Evaluate on train set after weight updates\n        train_loss,_,train_f1 = evaluate_BIO_tagger(model, train_loader)\n\n        # Validation Phase\n        val_loss,_,val_f1 = evaluate_BIO_tagger(model, val_loader)\n\n        # Log to WandB (only Loss and F1-score)\n        wandb.log({\n            \"Train Loss\": train_loss,\n            \"Train F1-score\": train_f1,\n            \"Val Loss\": val_loss,\n            \"Val F1-score\": val_f1\n        })\n\n        print(f\"Epoch [{epoch+1}/{epochs}] -> Train Loss: {train_loss:.4f}, Train F1: {train_f1:.4f} | Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}\")\n        \n    print(f\"--------------------------------------- TRAINING COMPLETED -------------------------------------\")\n    wandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:31:50.001056Z","iopub.execute_input":"2025-03-16T10:31:50.001323Z","iopub.status.idle":"2025-03-16T10:31:59.366272Z","shell.execute_reply.started":"2025-03-16T10:31:50.001302Z","shell.execute_reply":"2025-03-16T10:31:59.365347Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mharshu04\u001b[0m (\u001b[33mharshu04-indraprastha-institute-of-information-technolog\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"!pip install conlleval --quiet\nfrom conlleval import evaluate \nimport pprint\n\ndef evaluate_BIO_tagger(model, data_loader, verbose=False):\n    \"\"\"\n    Evaluate a trained model on a dataset for BIO tagging.\n\n    Args:\n        model (nn.Module): Trained PyTorch model.\n        data_loader (DataLoader): DataLoader for the dataset (test/validation).\n        verbose (bool): Whether to print the evaluation table.\n        \n    Returns:\n        tuple: A tuple containing (loss, tag-level F1, chunk-level F1)\n    \"\"\"\n    model.eval()  # Set to evaluation mode\n    total_loss = 0.0\n    total_tokens = 0  # Track total number of valid tokens\n    criterion = nn.CrossEntropyLoss(ignore_index=ATE_Dataset.PADlabel)  # Ignore padding token in loss\n\n    # Create idx2tag\n    idx2tag = {label: tag for tag, label in ATE_Dataset.BIOlabel.items()} \n    \n    eval_data = []  # Input format for conlleval : \"# gold_tag pred_tag\" \n\n    with torch.no_grad():\n        for tokens, labels in data_loader:\n            tokens, labels = tokens.to(device), labels.to(device)\n\n            outputs = model(tokens)\n            loss = criterion(outputs.view(-1, outputs.shape[-1]), labels.view(-1))\n\n            # Count valid tokens (excluding padding)\n            valid_tokens = (labels != ATE_Dataset.PADlabel).sum().item()\n            total_loss += loss.item() * valid_tokens  # Scale loss correctly\n            total_tokens += valid_tokens  # Only count valid tokens\n\n            preds = torch.argmax(outputs, dim=-1)\n            mask = labels != ATE_Dataset.PADlabel  # Ignore padding tokens\n            \n            for i in range(labels.shape[0]):  # Iterate over batch\n                gold_tags = [idx2tag[idx.item()] for idx in labels[i][mask[i]]]\n                pred_tags = [idx2tag[idx.item()] for idx in preds[i][mask[i]]]\n                for gt, pt in zip(gold_tags, pred_tags):\n                    eval_data.append(f\"# {gt} {pt}\")\n    \n    # Compute final loss by averaging over all valid (non-padding) tokens\n    avg_loss = total_loss / total_tokens if total_tokens > 0 else 0.0 \n    \n    # Compute evaluation metrics using conlleval\n    results = evaluate(eval_data)\n    results['loss'] = avg_loss\n    tag_f1 = results['overall']['tags']['evals']['f1']\n    chunk_f1 = results['overall']['chunks']['evals']['f1']\n    \n    if verbose:\n        pprint.pprint(results)  \n\n    return avg_loss, tag_f1, chunk_f1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:31:59.368063Z","iopub.execute_input":"2025-03-16T10:31:59.368440Z","iopub.status.idle":"2025-03-16T10:32:04.290968Z","shell.execute_reply.started":"2025-03-16T10:31:59.368417Z","shell.execute_reply":"2025-03-16T10:32:04.289569Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import os \n\n# Create a folder for saving weights if it doesn't exist\nos.makedirs(\"models\", exist_ok=True)\n\nwandb.finish()\nfor EmbeddingObject_name, EmbeddingObject in zip([\"GloVe\", \"fastText\"], [GloVe, fastText]): \n    # Load datasets\n    train_dataset = ATE_Dataset(\"train_task_1.json\", EmbeddingObject)\n    val_dataset = ATE_Dataset(\"val_task_1.json\", EmbeddingObject)\n    # Create DataLoaders \n    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=ATE_Dataset.collate_fn)\n    val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, collate_fn=ATE_Dataset.collate_fn)\n\n    for ModelClass_name, ModelClass in zip([\"RNN\", \"GRU\"], [ATE_RNN, ATE_GRU]):\n        model_name = f\"{ModelClass_name}_{EmbeddingObject_name}\"\n        # Initialize model\n        model = ModelClass(EmbeddingObject.embeddings, hidden_dim=128, output_dim=len(ATE_Dataset.BIOlabel))\n        # Train the model\n        train(model, train_loader, val_loader, epochs=5, learning_rate=5e-4, model_name=model_name)\n        # Save the model\n        torch.save(model.state_dict(), f\"models/{model_name}.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:32:04.292432Z","iopub.execute_input":"2025-03-16T10:32:04.292715Z","iopub.status.idle":"2025-03-16T10:35:18.186328Z","shell.execute_reply.started":"2025-03-16T10:32:04.292690Z","shell.execute_reply":"2025-03-16T10:35:18.185640Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250316_103204-edmopgkq</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task1/runs/edmopgkq' target=\"_blank\">RNN_GloVe</a></strong> to <a href='https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task1' target=\"_blank\">https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task1</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task1/runs/edmopgkq' target=\"_blank\">https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task1/runs/edmopgkq</a>"},"metadata":{}},{"name":"stdout","text":"------------------------------------- TRAINING RNN_GloVe -------------------------------------\nEpoch [1/5] -> Train Loss: 0.1689, Train F1: 0.6660 | Val Loss: 0.1554, Val F1: 0.6675\nEpoch [2/5] -> Train Loss: 0.1511, Train F1: 0.6867 | Val Loss: 0.1397, Val F1: 0.6852\nEpoch [3/5] -> Train Loss: 0.1260, Train F1: 0.7492 | Val Loss: 0.1284, Val F1: 0.7050\nEpoch [4/5] -> Train Loss: 0.1216, Train F1: 0.7365 | Val Loss: 0.1275, Val F1: 0.7072\nEpoch [5/5] -> Train Loss: 0.1038, Train F1: 0.7747 | Val Loss: 0.1197, Val F1: 0.7181\n--------------------------------------- TRAINING COMPLETED -------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train F1-score</td><td>▁▂▆▆█</td></tr><tr><td>Train Loss</td><td>█▆▃▃▁</td></tr><tr><td>Val F1-score</td><td>▁▃▆▆█</td></tr><tr><td>Val Loss</td><td>█▅▃▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train F1-score</td><td>0.77467</td></tr><tr><td>Train Loss</td><td>0.10376</td></tr><tr><td>Val F1-score</td><td>0.71809</td></tr><tr><td>Val Loss</td><td>0.11974</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">RNN_GloVe</strong> at: <a href='https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task1/runs/edmopgkq' target=\"_blank\">https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task1/runs/edmopgkq</a><br> View project at: <a href='https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task1' target=\"_blank\">https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250316_103204-edmopgkq/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250316_103253-avstsgej</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task1/runs/avstsgej' target=\"_blank\">GRU_GloVe</a></strong> to <a href='https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task1' target=\"_blank\">https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task1</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task1/runs/avstsgej' target=\"_blank\">https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task1/runs/avstsgej</a>"},"metadata":{}},{"name":"stdout","text":"------------------------------------- TRAINING GRU_GloVe -------------------------------------\nEpoch [1/5] -> Train Loss: 0.1483, Train F1: 0.7055 | Val Loss: 0.1367, Val F1: 0.6960\nEpoch [2/5] -> Train Loss: 0.1221, Train F1: 0.7497 | Val Loss: 0.1281, Val F1: 0.7221\nEpoch [3/5] -> Train Loss: 0.0968, Train F1: 0.7901 | Val Loss: 0.1165, Val F1: 0.7239\nEpoch [4/5] -> Train Loss: 0.0763, Train F1: 0.8318 | Val Loss: 0.1142, Val F1: 0.7294\nEpoch [5/5] -> Train Loss: 0.0587, Train F1: 0.8732 | Val Loss: 0.1113, Val F1: 0.7380\n--------------------------------------- TRAINING COMPLETED -------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train F1-score</td><td>▁▃▅▆█</td></tr><tr><td>Train Loss</td><td>█▆▄▂▁</td></tr><tr><td>Val F1-score</td><td>▁▅▆▇█</td></tr><tr><td>Val Loss</td><td>█▆▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train F1-score</td><td>0.8732</td></tr><tr><td>Train Loss</td><td>0.05874</td></tr><tr><td>Val F1-score</td><td>0.73797</td></tr><tr><td>Val Loss</td><td>0.11128</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">GRU_GloVe</strong> at: <a href='https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task1/runs/avstsgej' target=\"_blank\">https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task1/runs/avstsgej</a><br> View project at: <a href='https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task1' target=\"_blank\">https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250316_103253-avstsgej/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250316_103340-nb8kjbb4</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task1/runs/nb8kjbb4' target=\"_blank\">RNN_fastText</a></strong> to <a href='https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task1' target=\"_blank\">https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task1</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task1/runs/nb8kjbb4' target=\"_blank\">https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task1/runs/nb8kjbb4</a>"},"metadata":{}},{"name":"stdout","text":"------------------------------------- TRAINING RNN_fastText -------------------------------------\nEpoch [1/5] -> Train Loss: 0.1539, Train F1: 0.6858 | Val Loss: 0.1347, Val F1: 0.6981\nEpoch [2/5] -> Train Loss: 0.1371, Train F1: 0.7130 | Val Loss: 0.1219, Val F1: 0.7135\nEpoch [3/5] -> Train Loss: 0.1278, Train F1: 0.7329 | Val Loss: 0.1224, Val F1: 0.7225\nEpoch [4/5] -> Train Loss: 0.1258, Train F1: 0.7425 | Val Loss: 0.1228, Val F1: 0.7202\nEpoch [5/5] -> Train Loss: 0.1117, Train F1: 0.7537 | Val Loss: 0.1198, Val F1: 0.7087\n--------------------------------------- TRAINING COMPLETED -------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train F1-score</td><td>▁▄▆▇█</td></tr><tr><td>Train Loss</td><td>█▅▄▃▁</td></tr><tr><td>Val F1-score</td><td>▁▅█▇▄</td></tr><tr><td>Val Loss</td><td>█▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train F1-score</td><td>0.75372</td></tr><tr><td>Train Loss</td><td>0.1117</td></tr><tr><td>Val F1-score</td><td>0.70872</td></tr><tr><td>Val Loss</td><td>0.11978</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">RNN_fastText</strong> at: <a href='https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task1/runs/nb8kjbb4' target=\"_blank\">https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task1/runs/nb8kjbb4</a><br> View project at: <a href='https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task1' target=\"_blank\">https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250316_103340-nb8kjbb4/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250316_103428-nkmyc6ha</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task1/runs/nkmyc6ha' target=\"_blank\">GRU_fastText</a></strong> to <a href='https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task1' target=\"_blank\">https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task1</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task1/runs/nkmyc6ha' target=\"_blank\">https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task1/runs/nkmyc6ha</a>"},"metadata":{}},{"name":"stdout","text":"------------------------------------- TRAINING GRU_fastText -------------------------------------\nEpoch [1/5] -> Train Loss: 0.1486, Train F1: 0.7061 | Val Loss: 0.1359, Val F1: 0.7036\nEpoch [2/5] -> Train Loss: 0.1299, Train F1: 0.7115 | Val Loss: 0.1187, Val F1: 0.7137\nEpoch [3/5] -> Train Loss: 0.1177, Train F1: 0.7365 | Val Loss: 0.1123, Val F1: 0.7328\nEpoch [4/5] -> Train Loss: 0.1120, Train F1: 0.7453 | Val Loss: 0.1116, Val F1: 0.7149\nEpoch [5/5] -> Train Loss: 0.0988, Train F1: 0.7955 | Val Loss: 0.1149, Val F1: 0.7378\n--------------------------------------- TRAINING COMPLETED -------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train F1-score</td><td>▁▁▃▄█</td></tr><tr><td>Train Loss</td><td>█▅▄▃▁</td></tr><tr><td>Val F1-score</td><td>▁▃▇▃█</td></tr><tr><td>Val Loss</td><td>█▃▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train F1-score</td><td>0.79551</td></tr><tr><td>Train Loss</td><td>0.09882</td></tr><tr><td>Val F1-score</td><td>0.73779</td></tr><tr><td>Val Loss</td><td>0.1149</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">GRU_fastText</strong> at: <a href='https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task1/runs/nkmyc6ha' target=\"_blank\">https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task1/runs/nkmyc6ha</a><br> View project at: <a href='https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task1' target=\"_blank\">https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250316_103428-nkmyc6ha/logs</code>"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"## Testing and Model Inference","metadata":{}},{"cell_type":"code","source":"# preprocess(\"/kaggle/input/nlp-a2/test.json\", \"test_task_1.json\")\n\n# for EmbeddingObject_name, EmbeddingObject in zip([\"GloVe\", \"fastText\"], [GloVe, fastText]): \n#     # Load datasets\n#     test_dataset = ATE_Dataset(\"test_task_1.json\", EmbeddingObject)\n#     # Create DataLoaders \n#     test_loader = DataLoader(test_dataset, batch_size=256, shuffle=True, collate_fn=ATE_Dataset.collate_fn)\n\n#     for ModelClass_name, ModelClass in zip([\"RNN\", \"GRU\"], [ATE_RNN, ATE_GRU]):\n#         model_name = f\"{ModelClass_name}_{EmbeddingObject_name}\"\n#         # Initialize model\n#         model = ModelClass(EmbeddingObject.embeddings, hidden_dim=128, output_dim=len(ATE_Dataset.BIOlabel))\n#         # Load  Weights\n#         weight_path = f\"models/{model_name}.pt\" \n#         model.load_state_dict(torch.load(weight_path, map_location=device, weights_only=True))\n#         model.to(device)\n\n#         print(f\"-------------------------------- Evaluating {model_name} --------------------------------\")\n#         evaluate_BIO_tagger(model, test_loader, verbose = True)        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:35:18.187229Z","iopub.execute_input":"2025-03-16T10:35:18.187538Z","iopub.status.idle":"2025-03-16T10:35:18.191120Z","shell.execute_reply.started":"2025-03-16T10:35:18.187503Z","shell.execute_reply":"2025-03-16T10:35:18.190288Z"}},"outputs":[],"execution_count":10}]}