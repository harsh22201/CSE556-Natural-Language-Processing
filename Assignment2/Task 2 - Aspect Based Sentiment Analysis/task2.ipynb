{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-16T10:55:29.057323Z",
     "iopub.status.busy": "2025-03-16T10:55:29.056944Z",
     "iopub.status.idle": "2025-03-16T10:55:29.160669Z",
     "shell.execute_reply": "2025-03-16T10:55:29.160008Z",
     "shell.execute_reply.started": "2025-03-16T10:55:29.057294Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def preprocess(input_file, output_file):\n",
    "    # Read the JSON file\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        input_data = json.load(f)\n",
    "    \n",
    "    output_data = []\n",
    "    \n",
    "    for sentence_data in input_data:\n",
    "        sentence = sentence_data['sentence'].lower() \n",
    "        sentence = re.sub(r'[^a-zA-Z0-9\\s]', ' ', sentence)  # Remove punctuation and special characters\n",
    "        tokens = sentence.split()\n",
    "        \n",
    "        # Process aspect terms\n",
    "        for aspect in sentence_data.get('aspect_terms', []):\n",
    "            polarity = aspect['polarity'].lower()\n",
    "            term = aspect['term'].lower()\n",
    "            term = re.sub(r'[^a-zA-Z0-9\\s]', ' ', term)  # Remove punctuation and special characters\n",
    "            aspect_term = term.split()\n",
    "            \n",
    "            index = tokens.index(aspect_term[0])  # Find first token index\n",
    "        \n",
    "            output_data.append({\n",
    "                'tokens': tokens,\n",
    "                'polarity': polarity,\n",
    "                'aspect_term': aspect_term,\n",
    "                'index': index,\n",
    "            })\n",
    "    \n",
    "    # Write to output JSON file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output_data, f, indent=4)\n",
    "\n",
    "\n",
    "preprocess(\"/kaggle/input/nlp-a2/train.json\", \"train_task_2.json\")\n",
    "preprocess(\"/kaggle/input/nlp-a2/val.json\", \"val_task_2.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Trained Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T10:55:29.161945Z",
     "iopub.status.busy": "2025-03-16T10:55:29.161720Z",
     "iopub.status.idle": "2025-03-16T10:55:29.168880Z",
     "shell.execute_reply": "2025-03-16T10:55:29.168034Z",
     "shell.execute_reply.started": "2025-03-16T10:55:29.161922Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class WordEmbeddings:\n",
    "    PAD_IDX = 0 \n",
    "    UNK_IDX = 1\n",
    "    def __init__(self, file_path, skip_first=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            file_path (str): Path to the word embedding file (GloVe or FastText).\n",
    "            skip_first (bool): Whether to skip the first line (needed for FastText).\n",
    "        \"\"\"\n",
    "        self.idx2word, self.embeddings = self.load_embeddings(file_path, skip_first)\n",
    "\n",
    "        # Add special tokens <PAD> (index = 0) and <UNK> (index = 1)\n",
    "        self.idx2word = [\"<PAD>\", \"<UNK>\"] + self.idx2word\n",
    "\n",
    "        # Convert embeddings to a tensor\n",
    "        self.embeddings = torch.as_tensor(self.embeddings, dtype=torch.float32)\n",
    "\n",
    "        # Generate word2idx mapping\n",
    "        self.word2idx = {word: idx for idx, word in enumerate(self.idx2word)}\n",
    "\n",
    "        # Append new embeddings (zeros for PAD, small random for UNK)\n",
    "        pad_embedding = torch.zeros((1, self.embeddings.shape[1]))  # <PAD> = all zeros\n",
    "        unk_embedding = torch.rand((1, self.embeddings.shape[1])) * 0.01  # Small random values for <UNK>\n",
    "        self.embeddings = torch.cat([pad_embedding, unk_embedding, self.embeddings], dim=0)\n",
    "\n",
    "    def load_embeddings(self, file_path, skip_first):\n",
    "        \"\"\"Loads embeddings from a file.\"\"\"\n",
    "        idx2word = []\n",
    "        embeddings = []\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            if skip_first:\n",
    "                next(f)  # Skip first line \n",
    "            for line in f:\n",
    "                values = line.strip().split()\n",
    "                word = values[0]\n",
    "                vector = list(map(float, values[1:]))\n",
    "                idx2word.append(word)\n",
    "                embeddings.append(vector)\n",
    "\n",
    "        return idx2word, embeddings\n",
    "\n",
    "    def get_embedding(self, word):\n",
    "        \"\"\"Returns the embedding for a given word or the <UNK> embedding if not found.\"\"\"\n",
    "        idx = self.word2idx.get(word, self.word2idx[\"<UNK>\"])\n",
    "        return self.embeddings[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the vocabulary size.\"\"\"\n",
    "        return len(self.idx2word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe and fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T10:55:29.170247Z",
     "iopub.status.busy": "2025-03-16T10:55:29.170018Z",
     "iopub.status.idle": "2025-03-16T10:56:07.712331Z",
     "shell.execute_reply": "2025-03-16T10:56:07.711642Z",
     "shell.execute_reply.started": "2025-03-16T10:55:29.170229Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "glove_path = \"/kaggle/input/nlp-a2/glove.6B/glove.6B.300d.txt\"\n",
    "GloVe = WordEmbeddings(glove_path, skip_first=False) \n",
    "\n",
    "# fasttext_path = \"/kaggle/input/nlp-a2/wiki-news-300d-1M.vec/wiki-news-300d-1M.vec\"\n",
    "# fastText = WordEmbeddings(fasttext_path, skip_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect Sentiment Classification (ASC) Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T10:56:07.713521Z",
     "iopub.status.busy": "2025-03-16T10:56:07.713306Z",
     "iopub.status.idle": "2025-03-16T10:56:07.730754Z",
     "shell.execute_reply": "2025-03-16T10:56:07.729988Z",
     "shell.execute_reply.started": "2025-03-16T10:56:07.713503Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ASC_Dataset(Dataset):\n",
    "    # Sentiment Label Mapping\n",
    "    sentiment_labels = {\"negative\": 0, \"neutral\": 1, \"positive\": 2, \"conflict\": 3}\n",
    "    \n",
    "    def __init__(self, filepath, word_embeddings):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            filepath (str): Path to the JSON file containing ABSA data.\n",
    "            word_embeddings (WordEmbeddings): Preloaded word embedding object.\n",
    "        \"\"\"\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.data = []\n",
    "\n",
    "        # Load JSON data\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            samples = json.load(f)\n",
    "\n",
    "        for sample in samples:\n",
    "            \n",
    "            tokens_idx = [\n",
    "                self.word_embeddings.word2idx.get(token, self.word_embeddings.word2idx[\"<UNK>\"])\n",
    "                for token in sample[\"tokens\"]\n",
    "            ]\n",
    "            aspect_idx = [\n",
    "                self.word_embeddings.word2idx.get(term, self.word_embeddings.word2idx[\"<UNK>\"])\n",
    "                for term in sample[\"aspect_term\"]\n",
    "            ]\n",
    "            polarity = ASC_Dataset.sentiment_labels[sample[\"polarity\"]]\n",
    "\n",
    "            self.data.append((torch.tensor(tokens_idx, dtype=torch.long),\n",
    "                              torch.tensor(aspect_idx, dtype=torch.long),\n",
    "                              torch.tensor(polarity, dtype=torch.long)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns token indices, aspect indices, and sentiment label for one sample.\"\"\"\n",
    "        return self.data[idx]\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        \"\"\"\n",
    "        Custom collate function to pad sequences to the max length in a batch.\n",
    "        \"\"\"\n",
    "        tokens, aspects, labels = zip(*batch)  # Unpack list of tuples\n",
    "\n",
    "        # Get max lengths\n",
    "        max_sentence_len = max(len(t) for t in tokens)\n",
    "        max_aspect_len = max(len(a) for a in aspects)\n",
    "\n",
    "        # Padding\n",
    "        tokens_padded = [torch.cat([t, torch.full((max_sentence_len - len(t),), WordEmbeddings.PAD_IDX, dtype=torch.long)]) for t in tokens]\n",
    "        aspects_padded = [torch.cat([a, torch.full((max_aspect_len - len(a),), WordEmbeddings.PAD_IDX, dtype=torch.long)]) for a in aspects]\n",
    "\n",
    "        return torch.stack(tokens_padded), torch.stack(aspects_padded), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T11:48:48.432056Z",
     "iopub.status.busy": "2025-03-16T11:48:48.431745Z",
     "iopub.status.idle": "2025-03-16T11:48:48.440895Z",
     "shell.execute_reply": "2025-03-16T11:48:48.440035Z",
     "shell.execute_reply.started": "2025-03-16T11:48:48.432029Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ATAE_LSTM(nn.Module):\n",
    "    def __init__(self, pretrained_embeddings, hidden_dim, output_dim):\n",
    "        super(ATAE_LSTM, self).__init__()\n",
    "        \n",
    "        # Load pretrained embeddings\n",
    "        vocab_size, embedding_dim = pretrained_embeddings.shape\n",
    "        aspect_dim = embedding_dim\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True, padding_idx=WordEmbeddings.PAD_IDX)\n",
    "\n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim + aspect_dim, \n",
    "                            hidden_size=hidden_dim, \n",
    "                            batch_first=True, \n",
    "                            bidirectional=True)\n",
    "\n",
    "        # Attention Mechanism\n",
    "        self.attention_M = nn.Linear(hidden_dim * 2 + aspect_dim, hidden_dim * 2 + aspect_dim)\n",
    "        self.attention_alpha = nn.Linear(hidden_dim * 2 + aspect_dim, 1)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)  # Added layer\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  # Final output layer\n",
    "\n",
    "    def forward(self, sentence, aspect):\n",
    "        # Embed sentence and aspect\n",
    "        sentence_emb = self.embedding(sentence)  # (batch, seq_len, embedding_dim)\n",
    "        aspect_emb = self.embedding(aspect)  # (batch, aspect_len, embedding_dim)\n",
    "\n",
    "        # Create aspect mask (1 for real tokens, 0 for padding)\n",
    "        aspect_mask = (aspect != WordEmbeddings.PAD_IDX).float()  # (batch, aspect_len)\n",
    "\n",
    "        # Compute mean of aspect embeddings, ignoring padding\n",
    "        aspect_emb = (aspect_emb * aspect_mask.unsqueeze(-1)).sum(dim=1) / aspect_mask.sum(dim=1, keepdim=True)  # (batch, embedding_dim)\n",
    "\n",
    "        # Expand aspect embedding across sentence length\n",
    "        aspect_expanded = aspect_emb.unsqueeze(1).expand(-1, sentence_emb.size(1), -1)  # (batch, seq_len, aspect_dim)\n",
    "\n",
    "        # Concatenate sentence embedding and aspect embedding\n",
    "        lstm_input = torch.cat((sentence_emb, aspect_expanded), dim=2)  # (batch, seq_len, embedding_dim + aspect_dim)\n",
    "\n",
    "        # LSTM Forward Pass\n",
    "        lstm_out, _ = self.lstm(lstm_input)  # (batch, seq_len, hidden_dim * 2)\n",
    "\n",
    "        # Compute Attention Scores\n",
    "        M_input = torch.cat((lstm_out, aspect_expanded), dim=2)  # (batch, seq_len, hidden_dim * 2 + aspect_dim)\n",
    "        M = torch.tanh(self.attention_M(M_input))  # (batch, seq_len, hidden_dim * 2 + aspect_dim)\n",
    "        attention_scores = self.attention_alpha(M).squeeze(-1)  # (batch, seq_len)\n",
    "\n",
    "        # Create sentence mask (1 for real tokens, 0 for padding)\n",
    "        sentence_mask = (sentence != WordEmbeddings.PAD_IDX).float()  # (batch, seq_len)\n",
    "\n",
    "        # Apply mask: Set padding tokens' scores to a very small value before normalization via softmax\n",
    "        attention_scores = attention_scores.masked_fill(sentence_mask == 0, -1e9)\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)  # (batch, seq_len)\n",
    "\n",
    "        # Compute context vector (weighted sum of LSTM outputs)\n",
    "        context = torch.sum(lstm_out * attention_weights.unsqueeze(-1), dim=1)  # (batch, hidden_dim * 2)\n",
    "\n",
    "        # Pass through fc1 + relu, then final layer\n",
    "        hidden = F.relu(self.fc1(context))  # (batch, hidden_dim)\n",
    "        output = self.fc2(hidden)  # (batch, output_dim)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T11:53:48.177065Z",
     "iopub.status.busy": "2025-03-16T11:53:48.176658Z",
     "iopub.status.idle": "2025-03-16T11:53:48.481215Z",
     "shell.execute_reply": "2025-03-16T11:53:48.480344Z",
     "shell.execute_reply.started": "2025-03-16T11:53:48.177025Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# Login to W&B\n",
    "user_secrets = UserSecretsClient()\n",
    "wandb_api = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "wandb.login(key=wandb_api)\n",
    "\n",
    "# Define Device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train(model, train_loader, val_loader, epochs, learning_rate, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Train ATAE-LSTM model for aspect-based sentiment classification.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The ATAE-LSTM model.\n",
    "        train_loader (DataLoader): DataLoader for training data.\n",
    "        val_loader (DataLoader): DataLoader for validation data.\n",
    "        epochs (int): Number of training epochs.\n",
    "        learning_rate (float): Learning rate.\n",
    "        model_name (str): Name of the model for logging.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize WandB\n",
    "    wandb.init(project=\"NLP_A2_Task2\", name=model_name)\n",
    "    wandb.config.update({\n",
    "        \"epochs\": epochs,\n",
    "        \"learning rate\": learning_rate,\n",
    "        \"batch_size\": train_loader.batch_size,\n",
    "        \"loss_function\": \"CrossEntropyLoss\",\n",
    "        \"optimizer\": \"Adam\"\n",
    "    })\n",
    "\n",
    "    model = model.to(device)  # Move model to GPU/CPU\n",
    "    criterion = nn.CrossEntropyLoss() \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    print(f\"---------------- TRAINING {model_name} ----------------\")\n",
    "    for epoch in range(epochs):\n",
    "        # Train Phase\n",
    "        model.train()\n",
    "\n",
    "        for sentence, aspect, labels in train_loader:\n",
    "            sentence, aspect, labels = sentence.to(device), aspect.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "            outputs, _ = model(sentence, aspect)  # Forward pass\n",
    "\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "        # Evaluate on train set \n",
    "        train_loss, train_acc = evaluate_ASC(model, train_loader)\n",
    "\n",
    "        # Evaluate on Validation set \n",
    "        val_loss, val_acc = evaluate_ASC(model, val_loader)\n",
    "\n",
    "        # Log to WandB (Loss and Accuracy)\n",
    "        wandb.log({\n",
    "            \"Train Loss\": train_loss,\n",
    "            \"Train Accuracy\": train_acc,\n",
    "            \"Val Loss\": val_loss,\n",
    "            \"Val Accuracy\": val_acc\n",
    "        })\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] -> Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    print(f\"---------------- TRAINING COMPLETED ----------------\")\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T10:56:16.687713Z",
     "iopub.status.busy": "2025-03-16T10:56:16.687330Z",
     "iopub.status.idle": "2025-03-16T10:56:16.694333Z",
     "shell.execute_reply": "2025-03-16T10:56:16.693363Z",
     "shell.execute_reply.started": "2025-03-16T10:56:16.687692Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_ASC(model, data_loader, verbose=False):\n",
    "    \"\"\"\n",
    "    Evaluate a trained model on a dataset for Aspect-Based Sentiment Classification (ASC).\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained PyTorch model (e.g., ATAE-LSTM).\n",
    "        data_loader (DataLoader): DataLoader for validation or test dataset.\n",
    "        verbose (bool): If True, prints accuracy details.\n",
    "\n",
    "    Returns:\n",
    "         average_loss (float): Average loss over the dataset.\n",
    "         average_accuracy (float): Accuracy of the model.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss, total_samples = 0, 0\n",
    "    criterion = nn.CrossEntropyLoss()  # CrossEntropyLoss for multi-class classification\n",
    "\n",
    "    preds, true_labels = [], []  # Store predictions and ground truths\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for sentence, aspect, labels in data_loader:\n",
    "            sentence, aspect, labels = sentence.to(device), aspect.to(device), labels.to(device)\n",
    "\n",
    "            outputs, _ = model(sentence, aspect)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "\n",
    "            # Convert logits to predicted class (argmax)\n",
    "            predicted_labels = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            # Append predictions and true labels\n",
    "            preds.extend(predicted_labels.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            # Update total loss\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    # Compute accuracy\n",
    "    preds = torch.tensor(preds)\n",
    "    true_labels = torch.tensor(true_labels)\n",
    "    accuracy = (preds == true_labels).float().mean().item()\n",
    "\n",
    "    # Print details if verbose\n",
    "    if verbose:\n",
    "        print(f\"Total Samples: {total_samples}\")\n",
    "        print(f\"Correct Predictions: {(preds == true_labels).sum().item()}\")\n",
    "        print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    average_loss = total_loss / total_samples\n",
    "\n",
    "    return average_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T12:11:13.905158Z",
     "iopub.status.busy": "2025-03-16T12:11:13.904850Z",
     "iopub.status.idle": "2025-03-16T12:11:40.711754Z",
     "shell.execute_reply": "2025-03-16T12:11:40.710476Z",
     "shell.execute_reply.started": "2025-03-16T12:11:13.905137Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250316_121114-lw5kqqyw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task2/runs/lw5kqqyw' target=\"_blank\">ATAE_LSTM</a></strong> to <a href='https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task2' target=\"_blank\">https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task2/runs/lw5kqqyw' target=\"_blank\">https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task2/runs/lw5kqqyw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- TRAINING ATAE_LSTM ----------------\n",
      "Epoch [1/25] -> Train Loss: 1.2587, Train Acc: 0.5863 | Val Loss: 1.2745, Val Acc: 0.5633\n",
      "Epoch [2/25] -> Train Loss: 1.0876, Train Acc: 0.5870 | Val Loss: 1.1285, Val Acc: 0.5633\n",
      "Epoch [3/25] -> Train Loss: 1.0339, Train Acc: 0.5870 | Val Loss: 1.0915, Val Acc: 0.5633\n",
      "Epoch [4/25] -> Train Loss: 1.0049, Train Acc: 0.5870 | Val Loss: 1.0731, Val Acc: 0.5633\n",
      "Epoch [5/25] -> Train Loss: 0.9807, Train Acc: 0.5876 | Val Loss: 1.0587, Val Acc: 0.5660\n",
      "Epoch [6/25] -> Train Loss: 0.9499, Train Acc: 0.5910 | Val Loss: 1.0406, Val Acc: 0.5660\n",
      "Epoch [7/25] -> Train Loss: 0.9122, Train Acc: 0.6086 | Val Loss: 1.0224, Val Acc: 0.5930\n",
      "Epoch [8/25] -> Train Loss: 0.8814, Train Acc: 0.6201 | Val Loss: 1.0137, Val Acc: 0.5957\n",
      "Epoch [9/25] -> Train Loss: 0.8421, Train Acc: 0.6424 | Val Loss: 1.0023, Val Acc: 0.6253\n",
      "Epoch [10/25] -> Train Loss: 0.8128, Train Acc: 0.6677 | Val Loss: 0.9923, Val Acc: 0.6038\n",
      "Epoch [11/25] -> Train Loss: 0.7896, Train Acc: 0.6863 | Val Loss: 0.9951, Val Acc: 0.6011\n",
      "Epoch [12/25] -> Train Loss: 0.7654, Train Acc: 0.6974 | Val Loss: 0.9822, Val Acc: 0.6119\n",
      "Epoch [13/25] -> Train Loss: 0.7440, Train Acc: 0.7062 | Val Loss: 0.9800, Val Acc: 0.6092\n",
      "Epoch [14/25] -> Train Loss: 0.7352, Train Acc: 0.7018 | Val Loss: 0.9843, Val Acc: 0.6280\n",
      "Epoch [15/25] -> Train Loss: 0.7034, Train Acc: 0.7210 | Val Loss: 0.9677, Val Acc: 0.6307\n",
      "Epoch [16/25] -> Train Loss: 0.6833, Train Acc: 0.7288 | Val Loss: 0.9651, Val Acc: 0.6253\n",
      "Epoch [17/25] -> Train Loss: 0.6656, Train Acc: 0.7443 | Val Loss: 0.9609, Val Acc: 0.6442\n",
      "Epoch [18/25] -> Train Loss: 0.6474, Train Acc: 0.7470 | Val Loss: 0.9614, Val Acc: 0.6307\n",
      "Epoch [19/25] -> Train Loss: 0.6276, Train Acc: 0.7582 | Val Loss: 0.9549, Val Acc: 0.6361\n",
      "Epoch [20/25] -> Train Loss: 0.6128, Train Acc: 0.7629 | Val Loss: 0.9667, Val Acc: 0.6415\n",
      "Epoch [21/25] -> Train Loss: 0.5997, Train Acc: 0.7781 | Val Loss: 0.9625, Val Acc: 0.6523\n",
      "Epoch [22/25] -> Train Loss: 0.5769, Train Acc: 0.7859 | Val Loss: 0.9608, Val Acc: 0.6550\n",
      "Epoch [23/25] -> Train Loss: 0.5566, Train Acc: 0.7970 | Val Loss: 0.9610, Val Acc: 0.6604\n",
      "Epoch [24/25] -> Train Loss: 0.5399, Train Acc: 0.8072 | Val Loss: 0.9647, Val Acc: 0.6685\n",
      "Epoch [25/25] -> Train Loss: 0.5242, Train Acc: 0.8116 | Val Loss: 0.9636, Val Acc: 0.6658\n",
      "---------------- TRAINING COMPLETED ----------------\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Accuracy</td><td>▁▁▁▁▁▁▂▂▃▄▄▄▅▅▅▅▆▆▆▆▇▇███</td></tr><tr><td>Train Loss</td><td>█▆▆▆▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>Val Accuracy</td><td>▁▁▁▁▁▁▃▃▅▄▄▄▄▅▅▅▆▅▆▆▇▇▇██</td></tr><tr><td>Val Loss</td><td>█▅▄▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train Accuracy</td><td>0.81155</td></tr><tr><td>Train Loss</td><td>0.52421</td></tr><tr><td>Val Accuracy</td><td>0.66577</td></tr><tr><td>Val Loss</td><td>0.96356</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ATAE_LSTM</strong> at: <a href='https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task2/runs/lw5kqqyw' target=\"_blank\">https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task2/runs/lw5kqqyw</a><br> View project at: <a href='https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task2' target=\"_blank\">https://wandb.ai/harshu04-indraprastha-institute-of-information-technolog/NLP_A2_Task2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250316_121114-lw5kqqyw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "# Create a folder for saving weights if it doesn't exist\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = ASC_Dataset(\"train_task_2.json\", GloVe)\n",
    "val_dataset = ASC_Dataset(\"val_task_2.json\", GloVe)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=ASC_Dataset.collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, collate_fn=ASC_Dataset.collate_fn)\n",
    "\n",
    "# Initialize model\n",
    "model = ATAE_LSTM(GloVe.embeddings, hidden_dim=128, output_dim=len(ASC_Dataset.sentiment_labels))\n",
    "\n",
    "# Train the model\n",
    "train(model, train_loader, val_loader, epochs=25, learning_rate=0.00003, model_name=\"ATAE_LSTM\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"models/ATAE_LSTM.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T10:58:40.984701Z",
     "iopub.status.busy": "2025-03-16T10:58:40.984291Z",
     "iopub.status.idle": "2025-03-16T10:58:40.988159Z",
     "shell.execute_reply": "2025-03-16T10:58:40.987453Z",
     "shell.execute_reply.started": "2025-03-16T10:58:40.984680Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # preprocess test.json\n",
    "# preprocess(\"/kaggle/input/nlp-a2/test.json\", \"test_task_2.json\")\n",
    "\n",
    "# # Load test dataset\n",
    "# test_dataset = ASC_Dataset(\"test_task_2.json\", GloVe)\n",
    "\n",
    "# # Create DataLoader\n",
    "# test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, collate_fn=ASC_Dataset.collate_fn)\n",
    "\n",
    "# # Initialize model\n",
    "# model = ATAE_LSTM(GloVe.embeddings, hidden_dim=128, output_dim=len(ASC_Dataset.sentiment_labels))\n",
    "\n",
    "# # Load  Weights \n",
    "# model.load_state_dict(torch.load(\"models/ATAE_LSTM.pt\", map_location=device, weights_only=True))\n",
    "# model.to(device)\n",
    "\n",
    "# print(f\"-------------------------------- Evaluating {model_name} --------------------------------\")\n",
    "# evaluate_ASC(model, test_loader, verbose = True)        "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6782041,
     "sourceId": 10934341,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
